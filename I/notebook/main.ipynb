{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masinsko ucenje - projekat\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008\n",
    " \n",
    "Klasifikacija - PredviÄ‘anje ponovne hospitalizacije pacijenta sa dijabetesom na osnovu podataka sa inicijalne hospitalizacije\n",
    "\n",
    "1.   Priprema podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as scikit\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from math import ceil\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, mutual_info_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/diabetic_data.csv\"\n",
    "dataframe = pd.read_csv(path, low_memory=False, na_values=[\n",
    "                        \"?\", \"Unknown/Invalid\"])\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"readmitted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_col(df, treshold):\n",
    "    missing = df.isna()\n",
    "    missing_count = missing.sum()\n",
    "    total_rows=df.shape[0]\n",
    "    to_drop = []\n",
    "    for i in range(0, missing_count.count()):\n",
    "        if missing_count[i] / total_rows > treshold:\n",
    "            to_drop.append(i)\n",
    "    return to_drop\n",
    "\n",
    "for i in get_null_col(dataframe, 0.1):\n",
    "    print(dataframe.columns[i])\n",
    "\n",
    "#dataframe = dataframe.drop(dataframe.columns[to_drop], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"medical_specialty\"].fillna(\"Unknown\", inplace=True)\n",
    "dataframe[\"payer_code\"].fillna(\"Unknown\", inplace=True)\n",
    "dataframe[\"weight\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weigth_scale_mapper = {\n",
    "    \"[0-25)\": 0 \n",
    "    ,\"[25-50)\": 1\n",
    "    ,\"[50-75)\": 2\n",
    "    ,\"[75-100)\": 3\n",
    "    ,\"[100-125)\": 4\n",
    "    ,\"[125-150)\": 5\n",
    "    ,\"[150-175)\": 6\n",
    "    ,\"[175-200)\": 7\n",
    "    ,\">200\": 8\n",
    "}\n",
    "dataframe[\"weight\"].replace(weigth_scale_mapper, inplace=True)\n",
    "dataframe[\"weight\"].fillna(round(dataframe[\"weight\"].mean()), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe.shape)\n",
    "dataframe.dropna(inplace=True)\n",
    "to_drop = [\"encounter_id\",]\n",
    "dataframe.drop(\n",
    "    dataframe[to_drop],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_value_cols(df):\n",
    "    to_drop=[]\n",
    "    for col in df.columns:\n",
    "        if len(df[col].unique()) == 1:\n",
    "            print(col)\n",
    "            to_drop.append(col)\n",
    "    return to_drop\n",
    "\n",
    "dataframe.drop(get_one_value_cols(dataframe),axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number Of Rows In The Original DataFrame:\", len(dataframe))\n",
    "print(\"Number Of Rows After Deduping:\", len(dataframe.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnose mapping http://icd9cm.chrisendres.com/index.php?action=contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_mapper(value: str) -> int:\n",
    "    try:\n",
    "        int_value = int(float(value))\n",
    "        if int_value <= 139:\n",
    "            return 'Infectious and Parasitic'\n",
    "        elif int_value <= 239:\n",
    "            return 'Neoplasms'\n",
    "        elif int_value <= 279:\n",
    "            return 'Metabolic'\n",
    "        elif int_value <= 289:\n",
    "            return 'Blood'\n",
    "        elif int_value <= 319:\n",
    "            return 'Mental'\n",
    "        elif int_value <= 389:\n",
    "            return 'Nervous System'\n",
    "        elif int_value <= 459:\n",
    "            return 'Circulatory System'\n",
    "        elif int_value <= 519:\n",
    "            return 'Respiratory System'\n",
    "        elif int_value <= 579:\n",
    "            return 'Digestive System'\n",
    "        elif int_value <= 629:\n",
    "            return 'Genitourinary System'\n",
    "        elif int_value <= 679:\n",
    "            return 'Pregnancy'\n",
    "        elif int_value <= 709:\n",
    "            return 'Skin'\n",
    "        elif int_value <= 739:\n",
    "            return 'Musculoskeletal'\n",
    "        elif int_value <= 759:\n",
    "            return 'Congenital'\n",
    "        elif int_value <= 779:\n",
    "            return 'Perinatal Period'\n",
    "        elif int_value <= 799:\n",
    "            return 'Ill Defined'\n",
    "        else:\n",
    "            return 'Injury'\n",
    "    except ValueError:\n",
    "        if value[0] == 'V':\n",
    "            return 'Status'\n",
    "        else:  # E\n",
    "            return 'Cause'\n",
    "\n",
    "\n",
    "dataframe['diag_1_class'] = dataframe['diag_1'].apply(lambda x: diag_mapper(x))\n",
    "dataframe['diag_2_class'] = dataframe['diag_2'].apply(lambda x: diag_mapper(x))\n",
    "dataframe['diag_3_class'] = dataframe['diag_3'].apply(lambda x: diag_mapper(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glu_scale_mapper = {\n",
    "    'None': 0,\n",
    "    'Norm': 1,\n",
    "    '>200': 2,\n",
    "    '>300': 3\n",
    "}\n",
    "a1_scale_mapper = {\n",
    "    'None': 0,\n",
    "    'Norm': 1,\n",
    "    '>7': 2,\n",
    "    '>8': 3,\n",
    "}\n",
    "\n",
    "one_hot = LabelBinarizer()\n",
    "lab_enc = LabelEncoder()\n",
    "\n",
    "dataframe['admission_type_id'] = dataframe['admission_type_id'].astype('category')\n",
    "dataframe['discharge_disposition_id'] = dataframe['discharge_disposition_id'].astype('category')\n",
    "dataframe['admission_source_id'] = dataframe['admission_source_id'].astype('category')\n",
    "dataframe['weight'] = dataframe['weight'].astype('int32')\n",
    "\n",
    "dataframe['age']=lab_enc.fit_transform(dataframe['age'])\n",
    "dataframe['gender'] = one_hot.fit_transform(dataframe['gender'])\n",
    "dataframe['max_glu_serum'].replace(glu_scale_mapper, inplace=True)\n",
    "dataframe['A1Cresult'].replace(a1_scale_mapper, inplace=True)\n",
    "dataframe['medical_specialty']=lab_enc.fit_transform(dataframe['medical_specialty'])\n",
    "\n",
    "lab_coded = ['metformin','repaglinide','nateglinide','chlorpropamide','glimepiride','acetohexamide',\n",
    "    'glipizide','glyburide','tolbutamide','pioglitazone','rosiglitazone','acarbose','miglitol','troglitazone',\n",
    "    'tolazamide','insulin','glyburide-metformin','glipizide-metformin','glimepiride-pioglitazone','metformin-pioglitazone'\n",
    "    ,'diag_1','diag_2','diag_3','payer_code']\n",
    "\n",
    "for col in lab_coded:\n",
    "    dataframe[col] = lab_enc.fit_transform(dataframe[col])\n",
    "\n",
    "dataframe['change'] = one_hot.fit_transform(dataframe['change'])\n",
    "dataframe['diabetesMed'] = one_hot.fit_transform(dataframe['diabetesMed'])\n",
    "dataframe['readmitted'] = lab_enc.fit_transform(dataframe['readmitted'])\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_label = dataframe\n",
    "dataframe_onehot = dataframe.copy(deep=True)\n",
    "dataframe_onehot_plus = dataframe.copy(deep=True)\n",
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_one_hot(df, columns):\n",
    "    tmp = pd.DataFrame()\n",
    "    for col in columns:\n",
    "        res = one_hot.fit_transform(df[col])\n",
    "        for val in range(0, len(res[0])):\n",
    "            new_col = res[:, val]\n",
    "            tmp_col = pd.DataFrame(\n",
    "                {col + '(' + str(one_hot.classes_[val])+')': new_col})\n",
    "            tmp = pd.concat((tmp, tmp_col), axis=1)\n",
    "    return pd.concat((df.reset_index(drop=True), tmp.reset_index(drop=True)), axis=1, join='inner')\n",
    "\n",
    "for col in ['diag_1_class', 'diag_2_class', 'diag_3_class','race']:\n",
    "    dataframe_label[col] = lab_enc.fit_transform(dataframe_label[col])\n",
    "\n",
    "dataframe_onehot_plus = df_one_hot(dataframe_onehot_plus, ['diag_1_class', 'diag_2_class', 'diag_3_class',\n",
    "                              'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'race'])\n",
    "dataframe_onehot_plus.drop(['diag_1_class', 'diag_2_class', 'diag_3_class', 'admission_type_id',\n",
    "                      'discharge_disposition_id', 'admission_source_id', 'race'], axis=1, inplace=True)\n",
    "\n",
    "dataframe_onehot = df_one_hot(dataframe_onehot, ['diag_1_class', 'diag_2_class', 'diag_3_class','race'])\n",
    "dataframe_onehot.drop(['diag_1_class', 'diag_2_class', 'diag_3_class','race'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print(dataframe_label.shape)\n",
    "print(dataframe_onehot_plus.shape)\n",
    "print(dataframe_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_variance_columns(df, target, treshold):\n",
    "\n",
    "    y = df[target]\n",
    "    X_cat = df.drop(columns=[target,])\n",
    "    selector = VarianceThreshold(threshold=treshold)\n",
    "    X_reduced = selector.fit_transform(X_cat, y)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    selected_columns = X_cat.iloc[:, cols].columns.tolist()\n",
    "    selected_columns.append(target)\n",
    "    print(X_reduced.shape)\n",
    "    return selected_columns\n",
    "\n",
    "\n",
    "# selected_columns = remove_low_variance_columns(\n",
    "#     dataframe_onehot, \"readmitted\", 0.005)\n",
    "\n",
    "# to_drop = []\n",
    "# print(\"deleted columns:\")\n",
    "\n",
    "# for col in dataframe_onehot.columns:\n",
    "#     if col not in selected_columns:\n",
    "#         print(col)\n",
    "#         to_drop.append(col)\n",
    "\n",
    "# # dataframe_onehot.drop(dataframe_onehot[to_drop], inplace=True, axis=1)\n",
    "# print(dataframe_onehot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_columns(df, target, best_ratio):\n",
    "\n",
    "    y = df[target]\n",
    "    X_cat = df.drop(columns=[target, 'diag_1', 'diag_2', 'diag_3'])\n",
    "    X = X_cat\n",
    "    selector = SelectKBest(mutual_info_classif, k=ceil(X.shape[1]*best_ratio))\n",
    "    X_reduced = selector.fit_transform(X, y)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    selected_columns = X.iloc[:, cols].columns.tolist()\n",
    "    selected_columns.append(target)\n",
    "\n",
    "    print(X_reduced.shape)\n",
    "\n",
    "    return selected_columns\n",
    "\n",
    "\n",
    "# selected = select_best_columns(dataframe_onehot, \"readmitted\", 0.8)\n",
    "\n",
    "# to_drop = []\n",
    "# print(\"deleted columns:\")\n",
    "\n",
    "# for col in dataframe_onehot.columns:\n",
    "#     if col not in selected:\n",
    "#         print(col)\n",
    "#         to_drop.append(col)\n",
    "\n",
    "# #dataframe_onehot.drop(dataframe_onehot[to_drop], inplace=True, axis=1)\n",
    "# print(dataframe_onehot.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.   Deskriptivna analiza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataframe_onehot['readmitted'].unique()\n",
    "y = dataframe_onehot['readmitted'].value_counts()\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(df):\n",
    "    target = 'readmitted'\n",
    "    y = df[target]\n",
    "    X = df.drop(columns=[target, 'diag_1', 'diag_2', 'diag_3'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=.2, random_state=1066)\n",
    "    encoder = ce.LeaveOneOutEncoder(return_df=True)\n",
    "    X_train_loo = encoder.fit_transform(X_train, y_train)\n",
    "    X_test_loo = encoder.transform(X_test)\n",
    "    model = GradientBoostingRegressor(\n",
    "        learning_rate=0.05, max_depth=5, n_estimators=500, min_samples_split=5, n_iter_no_change=10)\n",
    "    model.fit(X_train_loo, y_train)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test_loo)))\n",
    "    feature_importance = model.feature_importances_\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "    plt.barh(range(len(sorted_idx)),\n",
    "             feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "    plt.title('Feature Importance')\n",
    "\n",
    "    perm_importance = permutation_importance(\n",
    "        model, X_test_loo, y_test, n_repeats=10, random_state=1066)\n",
    "    sorted_idx = perm_importance.importances_mean.argsort()\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "    plt.barh(range(len(sorted_idx)),\n",
    "             perm_importance.importances_mean[sorted_idx], align='center')\n",
    "    plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "    plt.title('Permutation Importance')\n",
    "    \n",
    "\n",
    "# feature_importance(dataframe_onehot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Algoritmi za klasifikaciju\n",
    "\n",
    "k-Nearest Neighbors.\n",
    "Decision Trees.\n",
    "Naive Bayes.\n",
    "Random Forest.\n",
    "Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_onehot.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def gridSearch(df, target):\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    pipe = Pipeline([  # ('mms', MinMaxScaler()),\n",
    "        # ('knn', KNeighborsClassifier()),\n",
    "        (\"classifier\", RandomForestClassifier()),])\n",
    "\n",
    "    search_space = [\n",
    "        {\"classifier\": [LogisticRegression()],\n",
    "         \"classifier__penalty\": ['l2'],\n",
    "         \"classifier__C\": np.logspace(0, 4, 10)},\n",
    "        {\"classifier\": [RandomForestClassifier()],\n",
    "         \"classifier__n_estimators\": [10,100],#, 200\n",
    "         \"classifier__max_features\": [1, 2, 3]},\n",
    "        # {'knn__n_neighbors': [3, 5, 7, 9],\n",
    "        #     'knn__weights': ['uniform', 'distance'],\n",
    "        #     'knn__leaf_size': [15, 20]}\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "##Some metrics are essentially defined for binary classification tasks (e.g. f1_score, roc_auc_score)\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipe,\n",
    "                              param_grid=search_space,\n",
    "                              scoring=['accuracy','precision','average_precision',],#'roc_auc'\n",
    "                              refit='accuracy',\n",
    "                              cv=5,\n",
    "                              verbose=True,\n",
    "                            #   error_score='raise'\n",
    "                              )\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[target]).values,\n",
    "                                                        df[target].values, test_size=0.2 , random_state=42)\n",
    "\n",
    "    best_model = gridsearch.fit(X_train, y_train)\n",
    "\n",
    "    print(gridsearch.best_estimator_)\n",
    "\n",
    "    # print(gridSearch.cv_results_)\n",
    "\n",
    "    print(gridsearch.best_params_)\n",
    "\n",
    "    print(gridsearch.best_score_)\n",
    "\n",
    "\n",
    "    # df_result = pd.DataFrame(GS.cv_results_)\n",
    "    # df_result = df.sort_values(\"rank_test_accuracy\")\n",
    "    # print(df_result.head(10))\n",
    "\n",
    "\n",
    "    # best_model.best_estimator_.get_params()[\"classifier\"]\n",
    "\n",
    "\n",
    "    # print(gridsearch.score(X_train, y_train))\n",
    "\n",
    "\n",
    "gridSearch(dataframe_label, 'readmitted')\n",
    "\n",
    "\n",
    "     \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
